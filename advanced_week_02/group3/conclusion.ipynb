{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7be8c8-a79e-4a05-bf3d-ee2dcf2b0545",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Methods for outlier detection:\n",
    "\n",
    "- **Univariate methods**: Rely on evaluating each variable or principal component separately.  \n",
    "  - Z-score is a typical example: simple and interpretable, but may miss anomalies that only become visible when multiple variables act together.\n",
    "    \n",
    "- **Multivariate methods**: Evaluate samples based on the joint distribution of variables.  \n",
    "  - Distance-based approaches include score distance (Hotelling’s T², Mahalanobis) and reconstruction error, which measure how well a sample fits the PCA model.  \n",
    "  - Model-based approaches, such as Isolation Forest, capture irregular patterns directly without relying on covariance estimation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e8688-04e6-4635-8256-217a1aaf7871",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Is PCA necessary for outlier detection?\n",
    "\n",
    "- **For univariate methods**  \n",
    "  - Outliers in raw space may remain hidden when features are correlated.  \n",
    "  - After PCA, variance is concentrated and correlations are removed, making univariate detection more precise.  \n",
    "  - PCA is **highly beneficial** for univariate detection.  \n",
    "\n",
    "- **For multivariate distance-based methods**  \n",
    "  - Raw correlations can distort distance measures, leading to unstable detection.  \n",
    "  - PCA reduces this effect by orthogonalizing components, improving stability.  \n",
    "  - PCA is **recommended** for distance-based detection.  \n",
    "\n",
    "- **For multivariate model-based methods**  \n",
    "  - Advanced models (e.g., Isolation Forest) already account for interactions between variables. \n",
    "  - Their detection performance changes little with PCA transformation.  \n",
    "  - PCA is **optional** for model-based methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a5264-653d-4012-b24a-52bd666ec3ed",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Limitations of PCA for outlier detection\n",
    "\n",
    "- **Linear assumption**: PCA only captures linear correlations; outliers in non-linear structures may remain hidden.  \n",
    "- **Sensitivity to scaling and noise**: Without proper preprocessing, principal components may be distorted.  \n",
    "- **Threshold choice**: Cut-offs (e.g., 98% quantile) are somewhat arbitrary and may vary across datasets.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Future directions\n",
    "- **Robust PCA**: More resilient to noise and outliers during training, providing stable detection.  \n",
    "- **Hybrid methods**: Combine PCA with clustering, kernel methods, or ensemble techniques to capture non-linear outliers.  \n",
    "- **Better preprocessing**: Standardization, noise reduction, and feature engineering can improve detection quality.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

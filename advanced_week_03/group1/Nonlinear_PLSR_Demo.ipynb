{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Non-Linear Partial Least Squares Regression (PLSR)\n",
        "\n",
        "**Group Presentation \u2014 PhD Seminar**\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "- **Goal of regression:** predict a response variable `y` from explanatory variables `X`.\n",
        "- **Challenge:** when predictors are highly collinear or when the number of predictors >> observations.\n",
        "- **Partial Least Squares Regression (PLSR):**  \n",
        "  - Projects predictors into a lower-dimensional latent space.  \n",
        "  - Maximizes covariance between `X` (predictors) and `Y` (response).  \n",
        "  - Works well when predictors are correlated.  \n",
        "\n",
        "PLS is **linear by default**, but many real-world problems show **non-linear relationships**.  \n",
        "This notebook introduces PLS, its limitations, and extensions for **non-linear modeling**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. The Idea of PLS\n",
        "\n",
        "- PLS finds **latent components** (linear combinations of predictors).\n",
        "- These components maximize the covariance with the response variable.  \n",
        "- Compared to PCA:  \n",
        "  - PCA only maximizes variance in `X`.  \n",
        "  - PLS maximizes **covariance** between `X` and `Y`.\n",
        "\n",
        "Mathematically (simplified):\n",
        "\n",
        "$$\n",
        "\\text{Find weight vector } w \\text{ such that } t = Xw \\quad \\text{maximizes } \\text{Cov}(t, y).\n",
        "$$\n",
        "\n",
        "where `t` is a latent variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Linear PLSR Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Synthetic linear data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(200, 5)\n",
        "y = 3*X[:, 0] + 2*X[:, 1] + np.random.randn(200) * 0.1  # linear relation\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "pls = PLSRegression(n_components=2)\n",
        "pls.fit(X_train, y_train)\n",
        "y_pred = pls.predict(X_test)\n",
        "\n",
        "print(\"Linear PLS R2:\", r2_score(y_test, y_pred))\n",
        "\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.xlabel(\"True y\")\n",
        "plt.ylabel(\"Predicted y\")\n",
        "plt.title(\"Linear PLS Regression\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation:**  \n",
        "PLS performs well on linear data, handling multicollinearity and dimensionality reduction.  \n",
        "\n",
        "But what happens when the relationship is **non-linear**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Limitation: Non-linear Relationships"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic non-linear data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + 0.1*np.random.randn(200)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Apply linear PLS\n",
        "pls = PLSRegression(n_components=2)\n",
        "pls.fit(X_train, y_train)\n",
        "y_pred = pls.predict(X_test)\n",
        "\n",
        "print(\"Linear PLS on nonlinear data - R2:\", r2_score(y_test, y_pred))\n",
        "\n",
        "plt.scatter(X_test, y_test, label=\"True data\", alpha=0.7)\n",
        "plt.scatter(X_test, y_pred, label=\"Linear PLS predictions\", alpha=0.7)\n",
        "plt.legend()\n",
        "plt.title(\"Linear PLS fails on nonlinear patterns\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation:**  \n",
        "Linear PLS cannot capture the sine-wave structure. We need **non-linear extensions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Non-linear PLS Approaches\n",
        "\n",
        "Several strategies exist:\n",
        "\n",
        "1. **Polynomial PLS**  \n",
        "   - Expand features with polynomial terms.  \n",
        "   - Capture non-linear interactions.  \n",
        "\n",
        "2. **Kernel PLS**  \n",
        "   - Use kernel trick (similar to SVM).  \n",
        "   - Map data into higher-dimensional space.  \n",
        "\n",
        "3. **Spline-based PLS**  \n",
        "   - Model smooth non-linear functions.  \n",
        "\n",
        "4. **Neural Network PLS (NN-PLS)**  \n",
        "   - Combine latent structure modeling with neural nets.  \n",
        "\n",
        "In this notebook, we\u2019ll demonstrate **Polynomial PLS**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Non-linear PLS Example (Polynomial Expansion)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Polynomial feature expansion\n",
        "poly = PolynomialFeatures(degree=5, include_bias=False)\n",
        "X_poly_train = poly.fit_transform(X_train)\n",
        "X_poly_test = poly.transform(X_test)\n",
        "\n",
        "pls_poly = PLSRegression(n_components=5)\n",
        "pls_poly.fit(X_poly_train, y_train)\n",
        "y_pred_poly = pls_poly.predict(X_poly_test)\n",
        "\n",
        "print(\"Non-linear (Poly) PLS R2:\", r2_score(y_test, y_pred_poly))\n",
        "\n",
        "plt.scatter(X_test, y_test, label=\"True data\", alpha=0.7)\n",
        "plt.scatter(X_test, y_pred_poly, label=\"Poly-PLS predictions\", alpha=0.7)\n",
        "plt.legend()\n",
        "plt.title(\"Polynomial PLS captures non-linear structure\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "- **PLS Regression**: robust method for high-dimensional, collinear data.  \n",
        "- **Limitation**: standard PLS assumes linearity.  \n",
        "- **Extensions**: polynomial PLS, kernel PLS, neural network PLS.  \n",
        "- **Takeaway**: With feature engineering (e.g., polynomial expansion), PLS can model non-linear data effectively.  \n",
        "\n",
        "---\n",
        "\n",
        "## 8. References\n",
        "\n",
        "- Geladi, P., & Kowalski, B. R. (1986). *Partial least-squares regression: a tutorial*. Analytica Chimica Acta.  \n",
        "- Rosipal, R., & Kr\u00e4mer, N. (2006). *Overview and recent advances in partial least squares*. Springer.  \n",
        "- Martens, H., & N\u00e6s, T. (1989). *Multivariate Calibration*. Wiley.  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}